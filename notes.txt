
* fix random number generation per pixel
 * blue noise / low discrepancy likely not good options here since we want to use more samples instead of less
 ? have c++ side send an iterative sum of golden ratio to shader?
  * or try just adding the random number per frame to it!
   * i tried that and it looked aweful, maybe my prng sucks with seeds that are in 0-1?
  * try and see if you can keep the seed in 0-1?
  * if this works out get rid of app rand and frame number and app time i think.
  ! i tried this 8/29 at lunch and it didn't work out, not sure what the problem is.
  * white noise is probably better over area and time since you are doing more samples not fewer
 ! need to keep the rng seed from becoming so huge that small floats don't matter and it stops updating.
 ! lauren's seed handling could be decent! https://github.com/gheshu/gputracer/blob/master/src/depth.glsl#L43

* there's a bug where if you look too far upwards, it flips in a weird way. viewable in cornel box for instance, check it out.

* make your own texture(s)!
 * try erode / dilute in gimp.  Not sure how to do the "even brightness" thing the paper talks about though.
  * need a new crosshatch texture
 * need to figure out how to process the cross hatching textures. dilation filter, even color texture etc.
  * eventually write code that does this, or figure out steps in gimp.

! for color cross hatching, try YCoCg-R instead of YUV? https://en.wikipedia.org/wiki/YCoCg
 * maybe make "full brightness" be based on albedo somehow? even though it's not... shrug
 * color crosshatching:
  * convert to hue / brightness. hue is tint for texture, brightness selects texture
  * could also try using texture as a lerp value between a light and dark color
 * make it use textures for shading - using the color to tint the non black parts of the textures.

* get some fun models to use for testing, make some fun scenes.
 * Models at turbosquid! https://www.turbosquid.com/
  * and unity store
  * and here: https://github.com/movAX13h/HiOctaneTools/tree/master/Unpacked%20Assets/models
  * if using those high octane assets, need to texture them with atlas.png i think.

* it really would be nice if passing static branches would let you see names. Maybe a function that returns the correct type? Also make sure that all parameters are required. No silent failures!
 * like here in main.cpp
 * const CShader& shader = ShaderData::GetShader_showPathTrace({g_showGrey, g_showCrossHatch, g_smoothStep, g_aniso});
 * if i give too few arguments it's just fine with it. they are all anonymous too.

TODO's in code

Sean said he'd be willing to give me textures.
 * how bout models?

* could have a bounding shape on "instanced meshes" which if the ray hits, gives a start/stop triangle index range to test ray against.
 * could also possibly have a scale, translation, rotation for instance.
 * try only if actually needed.
 * should know when actual models used

* get rid of unused models and textures when done

* somehow there are leaks (d3d11 warning of live objects) on shutdown
 ? maybe due to static variables and no explicit destruction time of global auto release pointers

? does doing N samples per call instead of 1 make it run faster? decrease time spent on (texture) memory read / write?

================================= LATER IDEAS =================================

* could do a lens and get depth of field effect. (actually, won't work with this technique).. or will it? why wouldn't it work? (oh... trilinear mapping. would need to do lens flare as a post process!)
* reflections. Probably needs to be done as a post step, but could do N samples and jitter them even.
* anti aliasing? could render at 4x resolution and downsample final image (longer to render though)
 * or maybe we can jitter first bounce? not sure...
* caustics and refraction could be interesting
* Work with sean for the franklin booth technioque?

* in general, could "hrad code" scenes as hard coded hlsl code. not sure how much faster that would be if any
* add russian roulette to the path tracing? https://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer
 * lets you get the benefits of more bounces without paying the full cost and doesn't add bias

* bayesian integration would probably be better here since there's no specular. but there is occlusion so...
 * dunno how bayesian integration works. would be good to learn but maybe out of scope

* could also directly sample the light to make it converge faster (i think you should, if it's unbiased!)

* record movie support

* could also offset each axis a little bit seperately
 * these and more talked about here:
 * https://medium.com/@bgolus/normal-mapping-for-a-triplanar-shader-10bf39dca05a

================================= HIGH LEVEL GOALS =================================

1) path trace diffuse / emissive of a scene with a mesh. (monochromatic?)
2) use the brightness values to determine which tileable crosshatching texture to use (trilinear filtering w/ volume texture)
3) success!
4) maybe multiple scenes and some debug displays and such. (IMGUI? Or rely on renderdoc?)

================================= NOTES =================================

* implementing this: http://dl.acm.org/citation.cfm?id=3085054

DX11 guide: http://www.rastertek.com/dx11tut02.html

* when writing a CPU path tracer, each time i needed a random number, i just called rand() basically. very easy.
 * on gpu, not so easy to generate random numbers! (could show problems of doign it incorrectly)
 * ended up using a blue noise texture as initial seed, and adding frame number * golden ratio to make it a low discrepancy sequence over time.

* free blue nois textures: http://momentsingraphics.de/?p=127

* tiny obj loader: https://github.com/syoyo/tinyobjloader

* a similar, interesting technique: http://www.floored.com/blog/2014sketch-rendering/

* imgui: https://github.com/ocornut/imgui

@Jontology - saw some screenshots of my "stuck" results and had some ideas of what was going wrong.
* unreal: https://github.com/JonGreenberg/Crosshatching

* cross hatch textures could be a single color channel. Just didnt do that out of lazyness. Could fit 4 shades into one texture then. That's what the "floored" post does.

* this uses triplanar projection to make uv's.  If your object already has uv's, you probably would want to use those instead, to do 1 texture lookup per pixel instead of 3!
 * or is triplanar somehow better being based on object normals?

* the macro to reflect things are way handy until you have to debug them, then they are just a pain.
 * VS needs to make it easier to debug macros, by letting you step through expanded macro code perhaps.


 Questions for researcher:
  * when doing triplanar are the normals in world space or screen space?
  * taking a hand drawn (greyscale) image and making it constant brightness: ???