? in scene 3, why isn't the light more obvious?
 * maybe you need to compress the lights and darks?
 ? maybe w is calculated incorrectly?


* try some minor optimizations to see if they help much. (write details in notes for blog post i guess)
 * N samples per call, instead of just the 1.  may decrease time spent on (texture) memory read / write?
 * bounding volume (box? sphere?) as pre-test for rays for meshes.

 * on spheres there is a weird line on the textures, check it out (may be due to triplanar?)

! for color cross hatching, try YCoCg-R instead of YUV? https://en.wikipedia.org/wiki/YCoCg

* use D3DX11CreateTextureFromFile() to load textures so you can use PNG's etc

* need to figure out how to process the cross hatching textures. dilation filter, even color texture etc.
 * eventually write code that does this

? should source crosshatching textures be single channel? maybe doesn't matter

* color crosshatching:
 * convert to hue / brightness. hue is tint for texture, brightness selects texture
 * could also try using texture as a lerp value between a light and dark color

* Models at turbosquid! https://www.turbosquid.com/
 * and unity store
 * and here: https://github.com/movAX13h/HiOctaneTools/tree/master/Unpacked%20Assets/models
 * if using those high octane assets, need to texture them with atlas.png i think.

* get some fun models to use for testing, make some fun scenes.
* make it use textures for shading - using the color to tint the non black parts of the textures.

? have c++ side send an iterative sum of golden ratio to shader?
 * or try just adding the random number per frame to it!
  * i tried that and it looked aweful, maybe my prng sucks with seeds that are in 0-1?
 * try and see if you can keep the seed in 0-1?
 * if this works out get rid of app rand and frame number and app time i think.
 ! i tried this 8/29 at lunch and it didn't work out, not sure what the problem is.
 * white noise is probably better over area and time since you are doing more samples not fewer

* bayesian integration would probably be better here since there's no specular. but there is occlusion so...
 * dunno how bayesian integration works. would be good to learn but maybe out of scope

! the path on cornell box has red wall on left, but the "showpathtrace" flips it correctly to red wall on right. should fix that minor wrinkle

! need to keep the rng seed from becoming so huge that small floats don't matter and it stops updating.

show fps / mspf. maybe in title bar? also number of samples?

be able to move camera? reset sample count on camera move? WASD + mouse look?

test x64 and release

TODO's in code

Sean said he'd be willing to give me textures.

? I'm thinking we probably want window borders / close button on this window.

* make it spit out shader-error.txt even if there's just warnings?

* could also directly sample the light to make it converge faster (i think you should, if it's unbiased!)

* will eventually want to make videos probably. A thing to render a frame at a time and then use ffmpeg to combine? Would need logic for updating camera and objects over time. Fixed frame rate update obviously!

* could have a bounding shape on "instanced meshes" which if the ray hits, gives a start/stop triangle index range to test ray against.
 * could also possibly have a scale, translation, rotation for instance.

* get rid of unused models and textures when done

* add russian roulette to the path tracing? https://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer
 * lets you get the benefits of more bounces without paying the full cost and doesn't add bias

? could try jittering the first hit. anti alias may be fine shrug.

================================= LATER IDEAS =================================

* could do a lens and get depth of field effect. (actually, won't work with this technique).. or will it? why wouldn't it work? (oh... trilinear mapping. would need to do lens flare as a post process!)
* reflections. Probably needs to be done as a post step, but could do N samples and jitter them even.
* anti aliasing? could render at 4x resolution and downsample final image (longer to render though)
* caustics and refraction could be interesting
* Work with sean for the franklin booth technioque?

================================= HIGH LEVEL GOALS =================================

1) path trace diffuse / emissive of a scene with a mesh. (monochromatic?)
2) use the brightness values to determine which tileable crosshatching texture to use (trilinear filtering w/ volume texture)
3) success!
4) maybe multiple scenes and some debug displays and such. (IMGUI? Or rely on renderdoc?)

================================= NOTES =================================

DX11 guid: http://www.rastertek.com/dx11tut02.html

* when writing a CPU path tracer, each time i needed a random number, i just called rand() basically. very easy.
 * on gpu, not so easy to generate random numbers! (could show problems of doign it incorrectly)
 * ended up using a blue noise texture as initial seed, and adding frame number * golden ratio to make it a low discrepancy sequence over time.

* free blue nois textures: http://momentsingraphics.de/?p=127