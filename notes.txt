* make your own texture(s)!
 * try erode / dilute in gimp.  Not sure how to do the "even brightness" thing the paper talks about though.
  * need a new crosshatch texture
 * need to figure out how to process the cross hatching textures. dilation filter, even color texture etc.
  * eventually write code that does this, or figure out steps in gimp.

* could do some mesh instancing where a model primitive transforms the incoming and outgoing ray / normals etc.
 * instead of making more triangles.

* make some better scenes that don't use models too. some spheres and boxes doing color bleed, shadows, multiple lights, etc.

* get some fun models to use for testing, make some fun scenes.
 * Models at turbosquid! https://www.turbosquid.com/
  * and unity store
  * and here: https://github.com/movAX13h/HiOctaneTools/tree/master/Unpacked%20Assets/models
  * if using those high octane assets, need to texture them with atlas.png i think.

! for color cross hatching, try YCoCg-R instead of YUV? https://en.wikipedia.org/wiki/YCoCg
 * maybe make "full brightness" be based on albedo somehow? even though it's not... shrug
 * color crosshatching:
  * convert to hue / brightness. hue is tint for texture, brightness selects texture
  * could also try using texture as a lerp value between a light and dark color
 * make it use textures for shading - using the color to tint the non black parts of the textures.

* it really would be nice if passing static branches would let you see names. Maybe a function that returns the correct type? Also make sure that all parameters are required. No silent failures!
 * like here in main.cpp
 * const CShader& shader = ShaderData::GetShader_showPathTrace({g_showGrey, g_showCrossHatch, g_smoothStep, g_aniso});
 * if i give too few arguments it's just fine with it. they are all anonymous too.

TODO's in code

Sean said he'd be willing to give me textures.
 * how bout models?

* get rid of unused models and textures when done

================================= LATER IDEAS =================================

* could do a lens and get depth of field effect. (actually, won't work with this technique).. or will it? why wouldn't it work? (oh... trilinear mapping. would need to do lens flare as a post process!)
* reflections. Probably needs to be done as a post step, but could do N samples and jitter them even.
* anti aliasing? could render at 4x resolution and downsample final image (longer to render though)
 * or maybe we can jitter first bounce? not sure...
* caustics and refraction could be interesting
* Work with sean for the franklin booth technioque?

* in general, could "hrad code" scenes as hard coded hlsl code. not sure how much faster that would be if any
* add russian roulette to the path tracing? https://computergraphics.stackexchange.com/questions/2316/is-russian-roulette-really-the-answer
 * lets you get the benefits of more bounces without paying the full cost and doesn't add bias

* bayesian integration would probably be better here since there's no specular. but there is occlusion so...
 * dunno how bayesian integration works. would be good to learn but maybe out of scope

* could also directly sample the light to make it converge faster (i think you should, if it's unbiased!)

* record movie support

* alternate idea: the texture could be thresholded instead of having slices. (From Jason)
 * but texture goes to 1bit basically instead of greyscale.
 * artists may also want control, like changing technique at different darkness levels.

* could also offset each axis a little bit seperately
 * these and more talked about here:
 * https://medium.com/@bgolus/normal-mapping-for-a-triplanar-shader-10bf39dca05a

================================= HIGH LEVEL GOALS =================================

1) path trace diffuse / emissive of a scene with a mesh. (monochromatic?)
2) use the brightness values to determine which tileable crosshatching texture to use (trilinear filtering w/ volume texture)
3) success!
4) maybe multiple scenes and some debug displays and such. (IMGUI? Or rely on renderdoc?)

================================= NOTES =================================

* implementing this: http://dl.acm.org/citation.cfm?id=3085054

DX11 guide: http://www.rastertek.com/dx11tut02.html

* when writing a CPU path tracer, each time i needed a random number, i just called rand() basically. very easy.
 * on gpu, not so easy to generate random numbers! (could show problems of doign it incorrectly)
 * ended up using a blue noise texture as initial seed, and adding frame number * golden ratio to make it a low discrepancy sequence over time.

* free blue nois textures: http://momentsingraphics.de/?p=127

* tiny obj loader: https://github.com/syoyo/tinyobjloader

* a similar, interesting technique: http://www.floored.com/blog/2014sketch-rendering/

* imgui: https://github.com/ocornut/imgui

@Jontology - saw some screenshots of my "stuck" results and had some ideas of what was going wrong.
* unreal: https://github.com/JonGreenberg/Crosshatching

* cross hatch textures could be a single color channel. Just didnt do that out of lazyness. Could fit 4 shades into one texture then. That's what the "floored" post does.

* this uses triplanar projection to make uv's.  If your object already has uv's, you probably would want to use those instead, to do 1 texture lookup per pixel instead of 3!
 * or is triplanar somehow better being based on object normals?

* the macro to reflect things are way handy until you have to debug them, then they are just a pain.
 * VS needs to make it easier to debug macros, by letting you step through expanded macro code perhaps.


 Questions for researcher:
  * when doing triplanar are the normals in world space or screen space?
  * taking a hand drawn (greyscale) image and making it constant brightness: ???